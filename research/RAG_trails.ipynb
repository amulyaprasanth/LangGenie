{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23b3f53-6602-4c6c-b715-a673d35e155d",
   "metadata": {},
   "source": [
    "# RAG Model Trails Application\n",
    "\n",
    "This notebook contains the trails and steps to creating an Application using Langchain and an opensource LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e404c2-326a-45fd-bd24-8d4aabdf3e0f",
   "metadata": {},
   "source": [
    "### Import the required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84cd013c-07a8-4c74-af8a-9e3a65e5cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0bf84-1ae2-4bda-b0e0-4e1628a896f4",
   "metadata": {},
   "source": [
    "### Load in PDF from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3fe611-bd9a-4720-a186-203b2bcd1710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pymupdf.open(r\"C:\\Users\\amuly\\OneDrive\\Desktop\\building-machine-learning-pipelines-automating-model-life-cycles-with-tensorflow-1nbsped-1492053198-9781492053194.pdf\")\n",
    "\n",
    "pages = []\n",
    "for page in doc: # Iterate through document pages\n",
    "        text = page.get_text() # get plain text (is in UTF-8)\n",
    "        pages.append(text)\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215e329-f22b-461a-bd3f-414029cc876d",
   "metadata": {},
   "source": [
    "### Create a text splitter and create a knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb826a3-5dad-429b-8ab9-852fe685b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents in the pdf \n",
    "split_docs = text_splitter.create_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793ced73-a795-440d-b0b5-6f77baeb906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a knowledge base\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "\n",
    "# Create a vector store and store the knowledge base\n",
    "knowledge_base = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e1508-399c-41dd-b460-2e46f627bef8",
   "metadata": {},
   "source": [
    "### Initialize the model and Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50bc8cba-dede-4565-ab00-53ae2659b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Prompt\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Provide a concise answer in 1-4 sentences:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60a628ad-09f2-495b-9845-b3a892fbdef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb197f-6629-4e2b-bff8-264080a683c4",
   "metadata": {},
   "source": [
    "### Create a chain and invoke it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb266421-d3a1-476e-9fb2-25dcf959249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pipeline orchestration refers to the process of managing and automating the flow of tasks within a machine learning (ML) pipeline. It involves scheduling, running, and monitoring multiple components or jobs that make up the pipeline, ensuring they execute reliably and efficiently. With automation, scalability, and reproducibility are key features, allowing data scientists to focus on experimentation rather than manual task management.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the knowledge base as retrever\n",
    "retriever = knowledge_base.as_retriever()\n",
    "\n",
    "# Create a RaG chain\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 return_source_documents=False,\n",
    "                                 chain_type_kwargs={\"prompt\": PROMPT})\n",
    "\n",
    "\n",
    "question = \"What is pipeline orchestration?\"\n",
    "\n",
    "qa(question)[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64182e12-0944-4bb4-a34a-e38e21b35a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
